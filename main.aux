\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{naturemag-doi}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {paragraph}{Junction state and network information flow.}{3}{Doc-Start}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Left -- Voltage magnitude and conductance across network junctions as a function of their current betweenness centrality at $t = 1.7\tmspace  +\thinmuskip {.1667em}$s. Right -- corresponding snapshot of network graph, showing the first current path (between orange nodes) established earlier, at $t = 1.7\tmspace  +\thinmuskip {.1667em}$s. Dashed edges denote junctions still in an "off" (high resistance) state.\relax }}{3}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:ebc+nw}{{1}{3}{Left -- Voltage magnitude and conductance across network junctions as a function of their current betweenness centrality at $t = 1.7\,$s. Right -- corresponding snapshot of network graph, showing the first current path (between orange nodes) established earlier, at $t = 1.7\,$s. Dashed edges denote junctions still in an "off" (high resistance) state.\relax }{figure.caption.1}{}}
\newlabel{fig:ebc+nw@cref}{{[figure][1][]1}{[1][3][]3}}
\@writefile{toc}{\contentsline {paragraph}{Transfer entropy -- information exchange.}{3}{figure.caption.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \textbf  {Might not need color here. Node in-TE as a function of centrality. (Out TE in supplement) ZK: I put these together. Can you make them the same scale to compare more easily, showing TE and cc increase at activation time.} 50 repetitions with different source-drain pairings on the same 100 nw network are plotted. Each data point represent one junction in one repetition. TE is averaged across the whole time-series. Node centrality is calculated based on the current-flow closeness algorithm at $t = 0$. Communicability is calculated by Eq \textbf  {equation} at $t = 0$ as well. All the source-drain pairings are controlled to have same graphical distance. Identical Mackey-Glass signals are applied to these simulations to activate the network. Each data point on the plot represent one node in one simulation. A close to linear correlation between TE and centrality can be determined from the plot, which means nodes with higher centralities tend to have richer dynamics for taking in and sending out information. \textbf  {Intrinsic property of the network that does not have any dynamics. We have structural information here.}\relax }}{4}{figure.caption.2}\protected@file@percent }
\newlabel{fig:TE_combined}{{2}{4}{\textbf {Might not need color here. Node in-TE as a function of centrality. (Out TE in supplement) ZK: I put these together. Can you make them the same scale to compare more easily, showing TE and cc increase at activation time.} 50 repetitions with different source-drain pairings on the same 100 nw network are plotted. Each data point represent one junction in one repetition. TE is averaged across the whole time-series. Node centrality is calculated based on the current-flow closeness algorithm at $t = 0$. Communicability is calculated by Eq \textbf {equation} at $t = 0$ as well. All the source-drain pairings are controlled to have same graphical distance. Identical Mackey-Glass signals are applied to these simulations to activate the network. Each data point on the plot represent one node in one simulation. A close to linear correlation between TE and centrality can be determined from the plot, which means nodes with higher centralities tend to have richer dynamics for taking in and sending out information. \textbf {Intrinsic property of the network that does not have any dynamics. We have structural information here.}\relax }{figure.caption.2}{}}
\newlabel{fig:TE_combined@cref}{{[figure][2][]2}{[1][4][]4}}
\@writefile{toc}{\contentsline {paragraph}{Network time series.}{4}{figure.caption.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \textbf  {Time series data for network measures.} The dashed vertical green line represents the time of first current path formation. \newline  (a) Collective conductance of network as a function of time. The collective conductance started increasing when the first junction is turned on. The growth will slow down after the first current path is formed. \newline  (b) The time derivative of conductance. The derivative is calculated using second order accurate central differences on the collective conductance time series data. The growth regime of conductance shows a great correspondence of $\Delta G$'s spikes. \newline  (c) Transfer entropy as a function of time. The transfer entropy time series data here is calculated with the Kraskov estimator and averaged across the network. A moving average with window size of 100 is applied to smooth the curve. Transfer entropy in the activation period is significantly higher than the rest. \newline  (d) Modularity as a function of time. The weighted modularity of network is calculated by Louvain method(Eq. \ref  {eq:mod}) As the network evolves, modularity will first increase as high conductance junctions are spread across the network and having their own communities. With the formation of the first current path, the modularity will have a significant drop since these isolating communities are connected. Then the modularity will keep having such fluctuations with smaller scales as more junctions are turned on and forming new current paths. \textbf  {Gaussian -linear inter, KSG - nonlinear, sensitive}\relax }}{5}{figure.caption.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \textbf  {Key events time as functions of first current path formation time.} The same 100 network is used for 50 simulations with different source-drain pairings. The graphical distances between sources and drains are controlled to be constant. Identical Mackey-Glass signal with random amplitudes ranging between 2-5 V are applied to these simulations. Simulations with higher amplitudes will form current paths earlier. \newline  X-axis represents the current path formation time in different simulations. Y-axis represents the corresponding time for key events to take place in these simulations. As shown in this figure, key events such as $\Delta G$ maximum, TE spike (in Gaussian estimation) and modularity drop will happen around the first current path formation time. \newline  \textbf  {Rephrase here} Thus the network will be optimal for information processing and specific task around the transition phase when the first current path is forming and the network is turning on. \relax }}{5}{figure.caption.3}\protected@file@percent }
\newlabel{fig:time_series}{{4}{5}{\textbf {Key events time as functions of first current path formation time.} The same 100 network is used for 50 simulations with different source-drain pairings. The graphical distances between sources and drains are controlled to be constant. Identical Mackey-Glass signal with random amplitudes ranging between 2-5 V are applied to these simulations. Simulations with higher amplitudes will form current paths earlier. \newline X-axis represents the current path formation time in different simulations. Y-axis represents the corresponding time for key events to take place in these simulations. As shown in this figure, key events such as $\Delta G$ maximum, TE spike (in Gaussian estimation) and modularity drop will happen around the first current path formation time. \newline \textbf {Rephrase here} Thus the network will be optimal for information processing and specific task around the transition phase when the first current path is forming and the network is turning on. \relax }{figure.caption.3}{}}
\newlabel{fig:time_series@cref}{{[figure][4][]4}{[1][4][]5}}
\@writefile{toc}{\contentsline {paragraph}{Network pre--activation -- memory capacity and learning}{6}{figure.caption.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces \textbf  {Network information flow snapshots at different time points.} Snapshots of the network are taken at time points before activation ($t = 0.5s$), when the first current path formation ($t=1.465s$), when the network finishes large scale activation ($t = 2s$) and when the network is stable with high collective conductance ($t = 2.5s$). Nodes are colored with time-averaged TE flow (inTE + outTE) within the last 0.2 second window using the Kraskov estimator. junctions are colored by the sum of corresponding transfer entropy in both directions. TEs are increasing when the network is being activated and richer dynamics emerge. After the network reaches a stable state (t = 2.5 s), the TEs activities decay as well.\relax }}{7}{figure.caption.4}\protected@file@percent }
\newlabel{fig:network_comparison}{{5}{7}{\textbf {Network information flow snapshots at different time points.} Snapshots of the network are taken at time points before activation ($t = 0.5s$), when the first current path formation ($t=1.465s$), when the network finishes large scale activation ($t = 2s$) and when the network is stable with high collective conductance ($t = 2.5s$). Nodes are colored with time-averaged TE flow (inTE + outTE) within the last 0.2 second window using the Kraskov estimator. junctions are colored by the sum of corresponding transfer entropy in both directions. TEs are increasing when the network is being activated and richer dynamics emerge. After the network reaches a stable state (t = 2.5 s), the TEs activities decay as well.\relax }{figure.caption.4}{}}
\newlabel{fig:network_comparison@cref}{{[figure][5][]5}{[1][6][]7}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces \textbf  {Benchmarks of the network with different initial states.} The network's voltage distribution and junction filament states are extracted every 10 time-step during the activation with a Mackey-Glass signal input. Memory capacity/ non-linear transformation tests are then performed accordingly initiating from those pre-activated states.   (a) Memory capacity result with respect to different initial states.  (b) Non-linear transformation performance with respect to different initial states.  (c) Average active information storage of memory capacity tests.  (d) Average transfer entropy of non-linear transformation tests.\relax }}{8}{figure.caption.5}\protected@file@percent }
\newlabel{fig:benchmark}{{6}{8}{\textbf {Benchmarks of the network with different initial states.} The network's voltage distribution and junction filament states are extracted every 10 time-step during the activation with a Mackey-Glass signal input. Memory capacity/ non-linear transformation tests are then performed accordingly initiating from those pre-activated states. \\ (a) Memory capacity result with respect to different initial states.\\ (b) Non-linear transformation performance with respect to different initial states.\\ (c) Average active information storage of memory capacity tests.\\ (d) Average transfer entropy of non-linear transformation tests.\relax }{figure.caption.5}{}}
\newlabel{fig:benchmark@cref}{{[figure][6][]6}{[1][6][]8}}
\citation{Dorfler2018}
\citation{Newman2010}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces \relax }}{9}{figure.caption.6}\protected@file@percent }
\newlabel{fig:nws}{{7}{9}{\relax }{figure.caption.6}{}}
\newlabel{fig:nws@cref}{{[figure][7][]7}{[1][9][]9}}
\newlabel{sec:level2}{{}{9}{}{figure.caption.6}{}}
\newlabel{sec:level2@cref}{{}{[1][9][]9}}
\citation{Brandes2005}
\citation{Estrada2008}
\citation{Crofts2009}
\citation{Rubinov2009}
\citation{Godwin2015}
\citation{Blondel2008}
\newlabel{eq:ebc}{{7}{10}{}{equation.0.7}{}}
\newlabel{eq:ebc@cref}{{[equation][7][]7}{[1][10][]10}}
\newlabel{eq:ecc}{{8}{10}{}{equation.0.8}{}}
\newlabel{eq:ecc@cref}{{[equation][8][]8}{[1][10][]10}}
\newlabel{eq:ncomm}{{10}{10}{}{equation.0.10}{}}
\newlabel{eq:ncomm@cref}{{[equation][10][]10}{[1][10][]10}}
\newlabel{eq:mod}{{11}{10}{}{equation.0.11}{}}
\newlabel{eq:mod@cref}{{[equation][11][]11}{[1][10][]10}}
\bibdata{paper1}
\bibcite{Dorfler2018}{1}
\bibcite{Newman2010}{2}
\bibcite{Brandes2005}{3}
\bibcite{Estrada2008}{4}
\bibcite{Crofts2009}{5}
\newlabel{eq:AIS}{{16}{11}{}{equation.0.16}{}}
\newlabel{eq:AIS@cref}{{[equation][16][]16}{[1][11][]11}}
\@writefile{toc}{\contentsline {section}{\hspace  *{-\tocsep }References}{11}{equation.0.19}\protected@file@percent }
\bibcite{Rubinov2009}{6}
\bibcite{Godwin2015}{7}
\bibcite{Blondel2008}{8}
\newlabel{paper1}{{}{12}{}{equation.0.19}{}}
\newlabel{paper1@cref}{{}{[1][12][]12}}
\newlabel{LastPage}{{}{12}{}{page.12}{}}
\xdef\lastpage@lastpage{12}
\xdef\lastpage@lastpageHy{12}
\ttl@finishall
